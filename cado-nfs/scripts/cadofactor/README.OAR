cadofactor and OAR HowTo:

OAR is the job scheduler used on various clusters, among them the clusters
of the Grid5000 research network, and on the Catrel cluster at LORIA.


1. Running cadofactor inside the OAR job:

This is arguably the easiest to set up. It allows cadofactor to get the list
of hostnames on which to launch slaves directly from OAR by using the
parameter

slaves.hostnames = @${OAR_NODE_FILE}

which reads the list of host names from the file specified in the shell
environment variable OAR_NODE_FILE. That file lists each multiple times, 
as often as there are virtual CPUs. As each process spawned by clients
usually uses more than one thread (i.e., as many as specified by the
threads parameter), it is sensible to use fewer clients per host than
the number of cores. For example, to use 8 clients per host:

slaves.nrclients = 8

Slaves on nodes managed by OAR need to be contacted via oarsh instead of ssh,
as ssh asks for the user's key passphrase whereas oarsh uses an automatically
generated key that is valid on all the nodes of the same job submission.
Using oarsh can be effected by

slaves.ssh.execbin = oarsh

A working example of parameters for OAR is in parameters.oar.

As oarsub expects the command to run as a single parameter, a wrapper script
needs to be used for starting cado-nfs.py with the required parameters;
alternatively an interactive session can be used in which to run cadofactor.

Of course, additional clients can be started on other OAR nodes manually
to help with the computation, assuming the hosts which are to help are
whitelisted in server.whitelist.


2. Running cadofactor outside the OAR job

If cadofactor runs on a machine outside the OAR-managed cluster, only the
slaves need to be started via OAR. They need the URL of the server, and if
the server uses SSL (which is the default), the server's certificate
fingerprint. A simple example shell script to launch slaves is in
start_clients.sh.



3. Full OAR integration with automatic job submission

This is TBD. The plan is:

A submission script is running on the OAR front end node and queries the
cadofactor script, running outside the cluster, for which task is currently
being processed. For each task, it has job submission parameters, giving the
number of nodes, number of clients per node, submission mode (besteffort or
not, etc.), etc. It submits jobs according to these parameters, monitors the
jobs, re-submits cancelled ones, or cancels them when the task for which
they were started has finished.

The individual submissions work pretty much like case 2. above; the
specified number of client scripts is run on each node; the client scripts
process work units.

To integrate the linear algebra task with this automatic submission, a
single workunit for the bwc.pl run will be generated, listing all the input
files for bwc.pl as URLs. bwc.pl has the capability of downloading its input
files via HTTP, so it can download them from the workunit server. It also
has the capability of automatically restarting an interrupted run, and of
querying OAR for submission parameters to parallelize/distribute its
operation automatically.

Thus, when the linear algebra step has started, the front end script will
submit a job and launch a single workunit client inside. That client
downloads the single Linear Algebra workunit, which starts bwc.pl. Then
bwc.pl downloads its inputs via HTTP from the workunit client, and runs the
linear algebra programs. The result files are uploaded to the workunit
server like for other workunits.
